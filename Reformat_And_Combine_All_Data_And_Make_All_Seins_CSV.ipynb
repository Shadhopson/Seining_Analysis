{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#google sheets datasets\n",
    "csv_2008 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=737788186&format=csv'\n",
    "csv_2009 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1188626592&format=csv'\n",
    "csv_2010 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1492092969&format=csv'\n",
    "csv_2011 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1154911519&format=csv'\n",
    "csv_2012 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1815364461&format=csv'\n",
    "csv_2013 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1417342758&format=csv'\n",
    "csv_2014 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1605738199&format=csv'\n",
    "csv_2015 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=0&format=csv'\n",
    "csv_2016 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1068275557&format=csv'\n",
    "csv_2017 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=126889157&format=csv'\n",
    "csv_2018 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=375464720&format=csv'\n",
    "#csv datasets\n",
    "#csv_2008 = 'Sein_Data/Seining Data 2008-2017 - 2008 Catch.csv'\n",
    "#csv_2009 = 'Sein_Data/Seining Data 2008-2017 - 2009 Catch.csv'\n",
    "#csv_2010 = 'Sein_Data/Seining Data 2008-2017 - 2010 Catch.csv'\n",
    "#csv_2011 = 'Sein_Data/Seining Data 2008-2017 - 2011 Catch.csv'\n",
    "#csv_2012 = 'Sein_Data/Seining Data 2008-2017 - 2012 Catch.csv'\n",
    "#csv_2013 = 'Sein_Data/Seining Data 2008-2017 - 2013 Catch.csv'\n",
    "#csv_2014 = 'Sein_Data/Seining Data 2008-2017 - 2014 Catch.csv'\n",
    "#csv_2015 = 'Sein_Data/Seining Data 2008-2017 - 2015 Catch.csv'\n",
    "#csv_2016 = 'Sein_Data/Seining Data 2008-2017 - 2016 Catch.csv'\n",
    "#csv_2017 = 'Sein_Data/Seining Data 2008-2017 - 2017 Catch.csv'\n",
    "\n",
    "#df_2008 =  pd.read_csv(csv_2013)\n",
    "#sheet = '1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g'\n",
    "#df_2012 = pd.read_csv('https://docs.google.com/spreadsheets/d/' + \n",
    "#                   sheet +\n",
    "#                   '/export?gid=1815364461&format=csv'\n",
    "#                  )\n",
    "#df_2008.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_recent(df):\n",
    "    df = df[['Date', 'Common Name', 'Quantity']]\n",
    "    curr_date = df.loc[1,'Date']\n",
    "    for i, date in df.iterrows():\n",
    "        if pd.isnull(df.loc[i, 'Date']):\n",
    "            df.loc[i, 'Date'] = curr_date\n",
    "        else:\n",
    "            df.loc[i, 'Date'] = df.loc[i, 'Date'].split(\" \", 1)[0]\n",
    "            try:\n",
    "                curr_date = datetime.strptime(df.loc[i, 'Date'], '%m/%d/%Y' )\n",
    "            except:\n",
    "                curr_date = datetime.strptime(df.loc[i, 'Date'], '%m/%d/%y' )\n",
    "            df.loc[i, 'Date'] = curr_date\n",
    "    df.rename(columns={\"Common Name\": \"Species\", \"Quantity\": \"Number Caught\"}, inplace=True)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_2017_extra = pd.read_csv('Sein_Data/Seining Data 2008-2017 - 2017 Catch.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_17 =reformat_recent(df_2017_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reformat_2012():\n",
    "\n",
    "    df_2012 = pd.read_csv(csv_2012)\n",
    "    columns = df_2012.columns.values\n",
    "    #Find columns with 2 // as with dates 03/30/15\n",
    "    date_columns = [date for date in columns if date.count('/') == 2 ]\n",
    "    df_list = []\n",
    "    \n",
    "    # for every date make a separate dataframe\n",
    "    for date_col_name in date_columns:\n",
    "        #get all species names and all values for that day\n",
    "        abridged_df = df_2012[['Name of Species', date_col_name]].head(50)\n",
    "        #make column of dates\n",
    "        try:\n",
    "            #print date_col_name\n",
    "            abridged_df['Date'] =  datetime.strptime(date_col_name, '%m/%d/%Y' )\n",
    "        #except:\n",
    "        #    abridged_df['Date'] =  datetime.strptime(date_col_name, '%m/%d/%y' )\n",
    "        except:\n",
    "            try:\n",
    "                abridged_df['Date'] =  datetime.strptime(date_col_name, '%m/%d/`%y' )\n",
    "            except:\n",
    "                abridged_df['Date'] =  datetime.strptime(date_col_name, '%m/%d/%y' )\n",
    "        #remove all rows where nothing was caught\n",
    "        abridged_df = abridged_df.dropna()\n",
    "        abridged_df.rename(columns={date_col_name: 'Number Caught', 'Name of Species': 'Species'}, inplace=True)\n",
    "        df_list.append(abridged_df)\n",
    "    #combine all the dataframes into one big one\n",
    "    sein_df_2012 = pd.concat(df_list)\n",
    "    return sein_df_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reformat_2011():\n",
    "    df_2011 = pd.read_csv(csv_2011)            \n",
    "    #set proper column indexes and remove junk rows\n",
    "    df_2011.columns = df_2011.iloc[1]\n",
    "    df_2011.reindex(df_2011.index.drop(1))\n",
    "    df_2011 = df_2011.drop([0,1,36])\n",
    "    #renaming columns to substitute for blank values '######' with approximations\n",
    "    df_2011.columns = ['Name of Species', '5/10/2011', '5/14/2011', '5/20/2011', '5/28/2011',\n",
    "           'Total Number May', '6/11/2011', '6/14/2011', '6/26/2011',\n",
    "           'Total Number June', '7/10/2011', '7/11/2011', '7/12/2011',\n",
    "           '7/13/2011', 'Total Number July', '8/02/2011', '8/9/2011',\n",
    "           '8/10/2011', 'Total Number August', '9/10/2011', '9/20/2011', '9/26/2011',\n",
    "           'Total Number Sept', 'Total Number October']\n",
    "    #remove columns that are just sums of past columns\n",
    "    selected_columns = [col for col in df_2011 if \"Total\" not in col]\n",
    "    df_2011 = df_2011[selected_columns]\n",
    "    df_2011.head()\n",
    "    df_list_2011 = []\n",
    "    for col in df_2011.columns.values:\n",
    "        if col == 'Name of Species':\n",
    "            continue\n",
    "        abridged_df = df_2011[['Name of Species', col]]\n",
    "        abridged_df['Date'] = datetime.strptime(col, '%m/%d/%Y' )\n",
    "        abridged_df = abridged_df.dropna()\n",
    "        abridged_df.rename(columns={col: 'Number Caught', 'Name of Species': 'Species'}, inplace=True)\n",
    "        df_list_2011.append(abridged_df)\n",
    "\n",
    "    sein_df_2011 = pd.concat(df_list_2011)\n",
    "    return sein_df_2011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reformat_2010():\n",
    "    df_2010 = pd.read_csv(csv_2010)\n",
    "    df_2010 = df_2010.drop([0])\n",
    "    curr_date = df_2010.loc[1]\n",
    "    for i, date in df_2010.iterrows():\n",
    "        if pd.isnull(df_2010.loc[i, 'Unnamed: 0']):\n",
    "            df_2010.loc[i, 'Unnamed: 0'] = curr_date\n",
    "        else:\n",
    "            try:\n",
    "                curr_date = datetime.strptime(df_2010.loc[i, 'Unnamed: 0'], '%A, %B %d, %Y' )\n",
    "            except:\n",
    "                date_string = df_2010.loc[i, 'Unnamed: 0'] + \", 2010\"\n",
    "                curr_date = datetime.strptime(date_string, '%A %B %d, %Y' )\n",
    "            df_2010.loc[i, 'Unnamed: 0'] = curr_date\n",
    "    df_2010 = df_2010[['Unnamed: 0', 'Seining catch 2010']]\n",
    "    df_2010 = df_2010.dropna()\n",
    "    df_2010 = df_2010.drop([1])\n",
    "    #rename columns\n",
    "    df_2010.rename(columns={\"Unnamed: 0\": \"Date\", \"Seining catch 2010\": \"Species\"}, inplace=True)\n",
    "    df_2010['Number Caught'] = np.NaN\n",
    "    #datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
    "    return df_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reformat_2009(): \n",
    "    df_2009 = pd.read_csv(csv_2009)\n",
    "    df_2009 = df_2009.drop([0,1,16])\n",
    "    curr_date = df_2009.loc[2]\n",
    "\n",
    "\n",
    "    for i, date in df_2009.iterrows():\n",
    "        if pd.isnull(df_2009.loc[i, 'Unnamed: 0']):\n",
    "            df_2009.loc[i, 'Unnamed: 0'] = curr_date\n",
    "        else:\n",
    "            try:\n",
    "                curr_date = datetime.strptime(df_2009.loc[i, 'Unnamed: 0'], '%A, %B %d, %Y' )\n",
    "            except:\n",
    "                date_string = df_2009.loc[i, 'Unnamed: 0'] + \", 2009\"\n",
    "                curr_date = datetime.strptime(date_string, '%A %B %d, %Y' )\n",
    "            df_2009.loc[i, 'Unnamed: 0'] = curr_date\n",
    "\n",
    "    df_2009 = df_2009[['Unnamed: 0', 'Seining catch 2009']]\n",
    "    df_2009 = df_2009.dropna()\n",
    "    #rename columns\n",
    "    df_2009.rename(columns={\"Unnamed: 0\": \"Date\", \"Seining catch 2009\": \"Species\"}, inplace=True)\n",
    "    df_2009['Number Caught'] = np.NaN\n",
    "    #datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
    "    return df_2009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reformat_2008():\n",
    "    df_2008 = pd.read_csv(csv_2008)\n",
    "    df_2008 = df_2008.drop([0,1])\n",
    "    curr_date = df_2008.loc[2]\n",
    "\n",
    "    for i, date in df_2008.iterrows():\n",
    "            if pd.isnull(df_2008.loc[i, 'Unnamed: 0']):\n",
    "                df_2008.loc[i, 'Unnamed: 0'] = curr_date\n",
    "            else:\n",
    "                try:\n",
    "                    curr_date = datetime.strptime(df_2008.loc[i, 'Unnamed: 0'], '%A, %B %d, %Y' )\n",
    "                except:\n",
    "                    date_string = df_2008.loc[i, 'Unnamed: 0'] + \", 2008\"\n",
    "                    curr_date = datetime.strptime(date_string, '%A %B %d, %Y' )\n",
    "                df_2008.loc[i, 'Unnamed: 0'] = curr_date\n",
    "\n",
    "    df_2008 = df_2008.drop([2])\n",
    "\n",
    "    df_2008 = df_2008[['Unnamed: 0', 'Seining catch 2008']]\n",
    "    df_2008 = df_2008.dropna()\n",
    "    #rename columns\n",
    "    df_2008.rename(columns={\"Unnamed: 0\": \"Date\", \"Seining catch 2008\": \"Species\"}, inplace=True)\n",
    "    df_2008['Number Caught'] = np.NaN\n",
    "    return df_2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sein_df_2008 = reformat_2008()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sein_df_2009 = reformat_2009()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sein_df_2010 = reformat_2010()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "sein_df_2011 = reformat_2011()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sein_df_2012 = reformat_2012()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:2754: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_2013 = pd.read_csv(csv_2013)\n",
    "\n",
    "df_2013.columns = df_2013.iloc[0]\n",
    "df_2013 = df_2013.drop([0])\n",
    "\n",
    "sein_df_2013 = reformat_recent(df_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_2014 = pd.read_csv(csv_2014)\n",
    "sein_df_2014 = reformat_recent(df_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "sein_df_2015 = reformat_recent(pd.read_csv(csv_2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "sein_df_2016 = reformat_recent(pd.read_csv(csv_2016))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "sein_df_2017 = reformat_recent(pd.read_csv(csv_2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "sein_df_2018 = reformat_recent(pd.read_csv(csv_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_seins = pd.concat([sein_df_2008,sein_df_2009,sein_df_2010,sein_df_2011,sein_df_2012, sein_df_2013,sein_df_2014, sein_df_2015, sein_df_2016,sein_df_2017,sein_df_2018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_seins = all_seins.dropna(subset=[\"Species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Number Caught</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sea squirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shore shrimp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mantis shrimp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sand shrimp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hermit crab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oyster toadfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>striped bass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>northern kingfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>northern pipefish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>striped searobin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlantic silverside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anchovy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blue crab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lady crab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>winter flounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>comb jelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2008-09-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lion's mane jellyfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Species caught</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bay anchovy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>northern pipefish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>striped bass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shore shrimp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sand shrimp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sea squirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blue crab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>comb jelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hydroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hermit crab,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>snails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2009-06-20 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two hemigrapsis molts were found on the beach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2018-09-21 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Periwinkle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2018-09-21 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>Atlantic Menhaden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2018-09-21 00:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Comb Jelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2018-09-21 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>Blue Crab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2018-09-21 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Tautog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2018-09-21 00:00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>Long Clawed Hermit Crab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2018-09-21 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Ribbed Mussel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Pipefish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>Silverside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Bay Anchovy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Silver Perch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Blue Crab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>Hermit Crab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Moon Jelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Comb Jelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>Mud Snail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Skilletfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>shore shrimp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2018-10-05 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Sand Shrimp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2018-10-06 00:00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>Bluefish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2018-10-06 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Atlantic Silverside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2018-10-06 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Porgy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2018-10-06 00:00:00</td>\n",
       "      <td>70</td>\n",
       "      <td>Comb Jelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2018-10-06 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Moon Jelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2018-10-06 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Mud Snail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2018-10-06 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Long Clawed Hermit Crab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2018-10-18 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Flounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2018-10-18 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>Striped Bass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2018-10-18 00:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>Silverside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2018-10-18 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Herring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1028 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Date Number Caught  \\\n",
       "3    2008-09-13 00:00:00           NaN   \n",
       "4    2008-09-13 00:00:00           NaN   \n",
       "5    2008-09-13 00:00:00           NaN   \n",
       "6    2008-09-13 00:00:00           NaN   \n",
       "7    2008-09-13 00:00:00           NaN   \n",
       "8    2008-09-13 00:00:00           NaN   \n",
       "9    2008-09-13 00:00:00           NaN   \n",
       "10   2008-09-13 00:00:00           NaN   \n",
       "11   2008-09-13 00:00:00           NaN   \n",
       "12   2008-09-13 00:00:00           NaN   \n",
       "13   2008-09-13 00:00:00           NaN   \n",
       "14   2008-09-13 00:00:00           NaN   \n",
       "15   2008-09-13 00:00:00           NaN   \n",
       "16   2008-09-13 00:00:00           NaN   \n",
       "17   2008-09-13 00:00:00           NaN   \n",
       "18   2008-09-13 00:00:00           NaN   \n",
       "19   2008-09-13 00:00:00           NaN   \n",
       "2    2009-06-20 00:00:00           NaN   \n",
       "3    2009-06-20 00:00:00           NaN   \n",
       "4    2009-06-20 00:00:00           NaN   \n",
       "5    2009-06-20 00:00:00           NaN   \n",
       "6    2009-06-20 00:00:00           NaN   \n",
       "7    2009-06-20 00:00:00           NaN   \n",
       "8    2009-06-20 00:00:00           NaN   \n",
       "9    2009-06-20 00:00:00           NaN   \n",
       "10   2009-06-20 00:00:00           NaN   \n",
       "11   2009-06-20 00:00:00           NaN   \n",
       "12   2009-06-20 00:00:00           NaN   \n",
       "13   2009-06-20 00:00:00           NaN   \n",
       "14   2009-06-20 00:00:00           NaN   \n",
       "..                   ...           ...   \n",
       "100  2018-09-21 00:00:00             1   \n",
       "101  2018-09-21 00:00:00             6   \n",
       "102  2018-09-21 00:00:00            17   \n",
       "103  2018-09-21 00:00:00             6   \n",
       "104  2018-09-21 00:00:00             3   \n",
       "105  2018-09-21 00:00:00            18   \n",
       "106  2018-09-21 00:00:00             1   \n",
       "107  2018-10-05 00:00:00             2   \n",
       "108  2018-10-05 00:00:00            12   \n",
       "109  2018-10-05 00:00:00             4   \n",
       "110  2018-10-05 00:00:00             4   \n",
       "111  2018-10-05 00:00:00             3   \n",
       "112  2018-10-05 00:00:00            10   \n",
       "113  2018-10-05 00:00:00             1   \n",
       "114  2018-10-05 00:00:00            17   \n",
       "115  2018-10-05 00:00:00             6   \n",
       "116  2018-10-05 00:00:00             1   \n",
       "117  2018-10-05 00:00:00             6   \n",
       "118  2018-10-05 00:00:00             4   \n",
       "119  2018-10-06 00:00:00            11   \n",
       "120  2018-10-06 00:00:00             5   \n",
       "121  2018-10-06 00:00:00             1   \n",
       "122  2018-10-06 00:00:00            70   \n",
       "123  2018-10-06 00:00:00             2   \n",
       "124  2018-10-06 00:00:00             2   \n",
       "125  2018-10-06 00:00:00             5   \n",
       "126  2018-10-18 00:00:00             1   \n",
       "127  2018-10-18 00:00:00             8   \n",
       "128  2018-10-18 00:00:00            13   \n",
       "129  2018-10-18 00:00:00             2   \n",
       "\n",
       "                                            Species  \n",
       "3                                        sea squirt  \n",
       "4                                      shore shrimp  \n",
       "5                                     mantis shrimp  \n",
       "6                                       sand shrimp  \n",
       "7                                       hermit crab  \n",
       "8                                   oyster toadfish  \n",
       "9                                      striped bass  \n",
       "10                                northern kingfish  \n",
       "11                                northern pipefish  \n",
       "12                                 striped searobin  \n",
       "13                              Atlantic silverside  \n",
       "14                                          anchovy  \n",
       "15                                        blue crab  \n",
       "16                                        lady crab  \n",
       "17                                  winter flounder  \n",
       "18                                       comb jelly  \n",
       "19                            lion's mane jellyfish  \n",
       "2                                    Species caught  \n",
       "3                                       bay anchovy  \n",
       "4                                 northern pipefish  \n",
       "5                                      striped bass  \n",
       "6                                      shore shrimp  \n",
       "7                                       sand shrimp  \n",
       "8                                       sea squirts  \n",
       "9                                         blue crab  \n",
       "10                                       comb jelly  \n",
       "11                                         hydroids  \n",
       "12                                     hermit crab,  \n",
       "13                                           snails  \n",
       "14   Two hemigrapsis molts were found on the beach.  \n",
       "..                                              ...  \n",
       "100                                      Periwinkle  \n",
       "101                               Atlantic Menhaden  \n",
       "102                                      Comb Jelly  \n",
       "103                                       Blue Crab  \n",
       "104                                          Tautog  \n",
       "105                         Long Clawed Hermit Crab  \n",
       "106                                   Ribbed Mussel  \n",
       "107                                        Pipefish  \n",
       "108                                      Silverside  \n",
       "109                                     Bay Anchovy  \n",
       "110                                    Silver Perch  \n",
       "111                                       Blue Crab  \n",
       "112                                     Hermit Crab  \n",
       "113                                      Moon Jelly  \n",
       "114                                      Comb Jelly  \n",
       "115                                       Mud Snail  \n",
       "116                                     Skilletfish  \n",
       "117                                    shore shrimp  \n",
       "118                                     Sand Shrimp  \n",
       "119                                        Bluefish  \n",
       "120                             Atlantic Silverside  \n",
       "121                                           Porgy  \n",
       "122                                      Comb Jelly  \n",
       "123                                      Moon Jelly  \n",
       "124                                       Mud Snail  \n",
       "125                         Long Clawed Hermit Crab  \n",
       "126                                        Flounder  \n",
       "127                                    Striped Bass  \n",
       "128                                      Silverside  \n",
       "129                                         Herring  \n",
       "\n",
       "[1028 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_seins.to_csv('Created CSVs/all_seins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
