{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan: Reformat to include Date, Time, Weathe & Air Temp, Water Temp, Tide Times, Tide is Moving, DO, PH, Salinity, Turbidity, # of Student Seiners, # of visitors, Instructors/ Seine Staff, Hauls, Common Name, Quantity,  # Brought to Ed Center, Other Info, Notes from seining season,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching to the 2017 version. No other season has all of these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_recent(df):\n",
    "    init_list = df.columns.values[:14]\n",
    "    curr_vals = df.loc[0][:14]\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(df.loc[i,'Date']):\n",
    "            df.loc[i] = list(curr_vals) + list(df.loc[i][14:])\n",
    "        else:\n",
    "            curr_vals = df.loc[i][:14]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_columns(curr_df, match_df):\n",
    "    for col in match_df.columns.values:\n",
    "        if col not in curr_df:\n",
    "            curr_df[col] = \"\"\n",
    "    return curr_df[match_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_2008 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=737788186&format=csv'\n",
    "csv_2009 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1188626592&format=csv'\n",
    "csv_2010 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1492092969&format=csv'\n",
    "csv_2011 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1154911519&format=csv'\n",
    "csv_2012 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1815364461&format=csv'\n",
    "csv_2013 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1417342758&format=csv'\n",
    "csv_2014 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1605738199&format=csv'\n",
    "csv_2015 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=0&format=csv'\n",
    "csv_2016 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=1068275557&format=csv'\n",
    "csv_2017 = 'https://docs.google.com/spreadsheets/d/1TH5vctylelJl_yHXJtiAnW48R4dEI-t_E-ObkWZ6-5g/export?gid=126889157&format=csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_df = reformat_recent(pd.read_csv(csv_2017))\n",
    "\n",
    "combined_df['Scientific Name'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_df =  combined_df[list(combined_df.columns.values[:14])+list(combined_df.columns.values[-1:]) + list(combined_df.columns.values[14:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2016 =  pd.read_csv(csv_2016)\n",
    "df_2016.rename(columns={\"Weather (exact air temp):\": 'Weather & Air Temp:', \"# student seiners (12+)\": '# student seiners (CLASSES ONLY)',\"# of visitors (Public Seine)\":'# of visitors (Public Seine ONLY)'}, inplace=True)\n",
    "df_2016 = add_columns(df_2016, combined_df)\n",
    "df_2016 = reformat_recent(df_2016)\n",
    "combined_df = pd.concat([df_2016,combined_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2015 =  pd.read_csv(csv_2015)\n",
    "df_2015.rename(columns={\"Weather (exact air temp):\": 'Weather & Air Temp:', \"#of Participants\":'# of visitors (Public Seine ONLY)', \"Instructors/Staff:\":\"Instructors/ Seine Staff:\", \"Other info (Dead/Alive)\":\"Other info (Gender, Length, Dead/Alive, other physical description)\"}, inplace=True)\n",
    "df_2015 = add_columns(df_2015, combined_df)\n",
    "df_2015 = reformat_recent(df_2015)\n",
    "combined_df = pd.concat([df_2015,combined_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2014 =  pd.read_csv(csv_2014)\n",
    "\n",
    "df_2014.rename(columns={\"Weather (exact air temp):\": 'Weather & Air Temp:', \"#of Participants\":'# of visitors (Public Seine ONLY)', \"Instructors/Staff:\":\"Instructors/ Seine Staff:\", \"Other info (Dead/Alive)\":\"Other info (Gender, Length, Dead/Alive, other physical description)\"}, inplace=True)\n",
    "df_2014 = add_columns(df_2014, combined_df)\n",
    "\n",
    "df_2014 = reformat_recent(df_2014)\n",
    "\n",
    "combined_df = pd.concat([df_2014,combined_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2013 =  pd.read_csv(csv_2013)\n",
    "df_2013.columns = list(df_2013.loc[0])\n",
    "df_2013 = df_2013[1:]\n",
    "\n",
    "weatherAndTemp = []\n",
    "for i,val in enumerate(list(df_2013['Weather:'])):\n",
    "    temp_val = list(df_2013[\"Air Temp:\"])[i]\n",
    "    if temp_val == temp_val:\n",
    "        weatherAndTemp.append(val + \" \" + temp_val)\n",
    "    else:\n",
    "        weatherAndTemp.append(val)\n",
    "\n",
    "\n",
    "df_2013['Weather & Air Temp:'] =  weatherAndTemp\n",
    "\n",
    "df_2013.rename(columns={ \"Instructors\":\"Instructors/ Seine Staff:\"}, inplace=True)\n",
    "df_2013 = df_2013.reset_index()\n",
    "df_2013 = add_columns(df_2013, combined_df)\n",
    "\n",
    "\n",
    "df_2013 = reformat_recent(df_2013)\n",
    "combined_df = pd.concat([df_2013,combined_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_2012():\n",
    "\n",
    "    df_2012 = pd.read_csv(csv_2012)\n",
    "    columns = df_2012.columns.values\n",
    "    #Find columns with 2 // as with dates 03/30/15\n",
    "    date_columns = [date for date in columns if date.count('/') == 2 ]\n",
    "    df_list = []\n",
    "    \n",
    "    # for every date make a separate dataframe\n",
    "    for date_col_name in date_columns:\n",
    "        #get all species names and all values for that day\n",
    "        abridged_df = df_2012[['Name of Species', date_col_name]].head(50)\n",
    "        abridged_df['Date']  = date_col_name\n",
    "        #remove all rows where nothing was caught\n",
    "        abridged_df = abridged_df.dropna()\n",
    "        abridged_df.rename(columns={date_col_name: 'Quantity', 'Name of Species': 'Common Name'}, inplace=True)\n",
    "        df_list.append(abridged_df)\n",
    "    #combine all the dataframes into one big one\n",
    "    sein_df_2012 = pd.concat(df_list)\n",
    "    return sein_df_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2012 = reformat_2012()\n",
    "\n",
    "df_2012 = add_columns(df_2012, combined_df)\n",
    "\n",
    "combined_df = pd.concat([df_2012,combined_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_2011():\n",
    "    df_2011 = pd.read_csv(csv_2011)            \n",
    "    #set proper column indexes and remove junk rows\n",
    "    df_2011.columns = df_2011.iloc[1]\n",
    "    df_2011.reindex(df_2011.index.drop(1))\n",
    "    df_2011 = df_2011.drop([0,1,36])\n",
    "    #renaming columns to substitute for blank values '######' with approximations\n",
    "    df_2011.columns = ['Name of Species', '5/10/2011', '5/14/2011', '5/20/2011', '5/28/2011',\n",
    "           'Total Number May', '6/11/2011', '6/14/2011', '6/26/2011',\n",
    "           'Total Number June', '7/10/2011', '7/11/2011', '7/12/2011',\n",
    "           '7/13/2011', 'Total Number July', '8/02/2011', '8/9/2011',\n",
    "           '8/10/2011', 'Total Number August', '9/10/2011', '9/20/2011', '9/26/2011',\n",
    "           'Total Number Sept', 'Total Number October']\n",
    "    #remove columns that are just sums of past columns\n",
    "    selected_columns = [col for col in df_2011 if \"Total\" not in col]\n",
    "    df_2011 = df_2011[selected_columns]\n",
    "    df_2011.head()\n",
    "    df_list_2011 = []\n",
    "    for col in df_2011.columns.values:\n",
    "        if col == 'Name of Species':\n",
    "            continue\n",
    "        abridged_df = df_2011[['Name of Species', col]]\n",
    "        abridged_df['Date'] = col#datetime.strptime(col, '%m/%d/%Y' )\n",
    "        abridged_df = abridged_df.dropna()\n",
    "        abridged_df.rename(columns={col: 'Quantity', 'Name of Species': 'Common Name'}, inplace=True)\n",
    "        df_list_2011.append(abridged_df)\n",
    "\n",
    "    sein_df_2011 = pd.concat(df_list_2011)\n",
    "    return sein_df_2011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_2011 = reformat_2011()\n",
    "\n",
    "df_2011 = add_columns(df_2011, combined_df)\n",
    "\n",
    "combined_df = pd.concat([df_2011,combined_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_2010():\n",
    "    df_2010 = pd.read_csv(csv_2010)\n",
    "    df_2010 = df_2010.drop([0])\n",
    "    curr_date = df_2010.loc[1]\n",
    "    for i, date in df_2010.iterrows():\n",
    "        if pd.isnull(df_2010.loc[i, 'Unnamed: 0']):\n",
    "            df_2010.loc[i, 'Unnamed: 0'] = curr_date\n",
    "        else:\n",
    "            curr_date = df_2010.loc[i, 'Unnamed: 0']\n",
    "    df_2010 = df_2010.drop([1])\n",
    "    #rename columns\n",
    "    df_2010.rename(columns={\"Unnamed: 0\": \"Date\", \"Seining catch 2010\": \"Common Name\", \"Unnamed: 2\":\"Other info (Gender, Length, Dead/Alive, other physical description)\"}, inplace=True)\n",
    "    df_2010 = df_2010.dropna(subset=['Common Name'])\n",
    "    return df_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2010 = reformat_2010()\n",
    "\n",
    "df_2010 =  add_columns(df_2010, combined_df)\n",
    "\n",
    "combined_df = pd.concat([df_2010,combined_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_2009(): \n",
    "    df_2009 = pd.read_csv(csv_2009)\n",
    "    df_2009 = df_2009.drop([0,1,16])\n",
    "    curr_date = df_2009.loc[2]\n",
    "\n",
    "    for i, date in df_2009.iterrows():\n",
    "        if pd.isnull(df_2009.loc[i, 'Unnamed: 0']):\n",
    "            df_2009.loc[i, 'Unnamed: 0'] = curr_date\n",
    "        else:\n",
    "            curr_date = df_2009.loc[i, 'Unnamed: 0']\n",
    "\n",
    "    df_2009 = df_2009[['Unnamed: 0', 'Seining catch 2009']]\n",
    "    df_2009 = df_2009.dropna()\n",
    "    #rename columns\n",
    "    df_2009.rename(columns={\"Unnamed: 0\": \"Date\", \"Seining catch 2009\": \"Common Name\"}, inplace=True)\n",
    "    #datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
    "    return df_2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2009 = reformat_2009()\n",
    "\n",
    "df_2009 =  add_columns(df_2009, combined_df)\n",
    "\n",
    "df_2009 = df_2009.drop([2])\n",
    "\n",
    "combined_df = pd.concat([df_2009,combined_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_2008():\n",
    "    df_2008 = pd.read_csv(csv_2008)\n",
    "    df_2008 = df_2008.drop([0,1])\n",
    "    curr_date = df_2008.loc[2]\n",
    "\n",
    "    for i, date in df_2008.iterrows():\n",
    "            if pd.isnull(df_2008.loc[i, 'Unnamed: 0']):\n",
    "                df_2008.loc[i, 'Unnamed: 0'] = curr_date\n",
    "            else:\n",
    "                curr_date = df_2008.loc[i, 'Unnamed: 0']\n",
    "\n",
    "    df_2008 = df_2008.drop([2])\n",
    "\n",
    "    df_2008 = df_2008[['Unnamed: 0', 'Seining catch 2008']]\n",
    "    df_2008 = df_2008.dropna()\n",
    "    #rename columns\n",
    "    df_2008.rename(columns={\"Unnamed: 0\": \"Date\", \"Seining catch 2008\": \"Common Name\"}, inplace=True)\n",
    "    return df_2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2008 =  reformat_2008()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2008 =  add_columns(df_2008, combined_df)\n",
    "combined_df = pd.concat([df_2008,combined_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_df = combined_df.apply(lambda x: x.str.strip()).replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# might not be giving na, could replace blanks with nan, and then switch back after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_df = combined_df.dropna(subset = [\"Common Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_df.to_csv('deleteme.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# October 2011 is all in a Totaled column, we probably want to include that in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4/25/2012 and 9/27/2012 and 10/4/2012 and 10/10/2012 didn't record the amount of fish caught\n",
    "#Got up to 2016"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
